{
 "metadata": {
  "name": "",
  "signature": "sha256:9f31a9ceb2c23ecccc0931572e14c63bff1e2d265a450469f7058126fe5f3cad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab\n",
      "==========================================\n",
      "Hadoop Streaming\n",
      "------------------------------------------\n",
      "Alessandro D. Gagliardi  \n",
      "*(adapted from [Michael G. Noll's tutorial](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/))*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Go to [console.aws.amazon.com/s3](https://console.aws.amazon.com/s3/home?region=us-east-1) and create a bucket for this course. Bucket names must be universally unique.  \n",
      "Try using the name of your github repo, but put it all in lower case as EMR may not like upper case in S3 buckets."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```bash\n",
      "$ pip install awscli\n",
      "$ aws configure\n",
      "```\n",
      "Use the same AWS Access Key ID and AWS Secret Access Key you used when configuring StarCluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Replace `<my_bucket>` with the name of your bucket:\n",
      "```bash\n",
      "$ aws s3 ls s3://<my_bucket>\n",
      "```\n",
      "Should return nothing (if there is nothing in there). If it returns an error, raise your hand!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "  ####$ history\n",
      " \n",
      "  504  pip install awscli\n",
      "  505  aws configure\n",
      "  506  aws s3 ls s3://cde-ds-ga\n",
      "  507  aws s3 cp pg20417.txt s3://cde-ds-ga/gutenberg/\n",
      "  508  aws s3 cp /Users/pachelbel/workshops/data-science/DAT_SF_12/classes/week3-Jan27-29/pg20417.txt s3://cde-ds-ga/gutenberg/\n",
      "  509  aws s3 cp /Users/pachelbel/workshops/data-science/DAT_SF_12/classes/week3-Jan27-29/pg5000.txt s3://cde-ds-ga/gutenberg/\n",
      "  510  aws s3 cp /Users/pachelbel/workshops/data-science/DAT_SF_12/classes/week3-Jan27-29/pg4300.txt s3://cde-ds-ga/gutenberg/\n",
      "  511  aws s3 ls s3://cde-ds-ga\n",
      "  512  aws s3 ls s3://cde-ds-ga/gutenberg/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Follow the instructions at https://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CLI_CreateStreaming.html to create an EMR cluster.  \n",
      "\n",
      "* Be sure to select an EC2 key pair.\n",
      "* Don't worry about adding a streaming step as we will be doing that manually."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mapper.py\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import sys\n",
      "\n",
      "def read_input(file):\n",
      "    for line in file:\n",
      "        # split the line into words\n",
      "        yield line.split()\n",
      "\n",
      "def main(separator='\\t'):\n",
      "    # input comes from STDIN (standard input)\n",
      "    data = read_input(sys.stdin)\n",
      "    for words in data:\n",
      "        # write the results to STDOUT (standard output);\n",
      "        # what we output here will be the input for the\n",
      "        # Reduce step, i.e. the input for reducer.py\n",
      "        #\n",
      "        # tab-delimited; the trivial word count is 1\n",
      "        for word in words:\n",
      "            print '%s%s%d' % (word, separator, 1)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting mapper.py\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file reducer.py\n",
      "#!/usr/bin/env python\n",
      "\n",
      "from itertools import groupby\n",
      "from operator import itemgetter\n",
      "import sys\n",
      "\n",
      "def read_mapper_output(file, separator='\\t'):\n",
      "    for line in file:\n",
      "        yield line.rstrip().split(separator, 1)\n",
      "\n",
      "def main(separator='\\t'):\n",
      "    # input comes from STDIN (standard input)\n",
      "    data = read_mapper_output(sys.stdin, separator=separator)\n",
      "    # groupby groups multiple word-count pairs by word,\n",
      "    # and creates an iterator that returns consecutive keys and their group:\n",
      "    #   current_word - string containing a word (the key)\n",
      "    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n",
      "    for current_word, group in groupby(data, itemgetter(0)):\n",
      "        try:\n",
      "            total_count = sum(int(count) for current_word, count in group)\n",
      "            print \"%s%s%d\" % (current_word, separator, total_count)\n",
      "        except ValueError:\n",
      "            # count was not a number, so silently discard this item\n",
      "            pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing reducer.py\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to make these files executable:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "chmod a+x mapper.py\n",
      "chmod a+x reducer.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's test the mapper phase:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "echo \"foo foo quux labs foo bar quux\" | ./mapper.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "foo\t1\n",
        "foo\t1\n",
        "quux\t1\n",
        "labs\t1\n",
        "foo\t1\n",
        "bar\t1\n",
        "quux\t1\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The shuffle phase works like `sort` in bash:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "echo \"foo foo quux labs foo bar quux\" | ./mapper.py | sort"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "bar\t1\n",
        "foo\t1\n",
        "foo\t1\n",
        "foo\t1\n",
        "labs\t1\n",
        "quux\t1\n",
        "quux\t1\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The whole pipeline:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "echo \"foo foo quux labs foo bar quux\" | ./mapper.py | sort | ./reducer.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "bar\t1\n",
        "foo\t3\n",
        "labs\t1\n",
        "quux\t2\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try it on one of our files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat pg20417.txt | ./mapper.py | head "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ufeffThe\t1\n",
        "Project\t1\n",
        "Gutenberg\t1\n",
        "EBook\t1\n",
        "of\t1\n",
        "The\t1\n",
        "Outline\t1\n",
        "of\t1\n",
        "Science,\t1\n",
        "Vol.\t1\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Traceback (most recent call last):\n",
        "  File \"./mapper.py\", line 23, in <module>\n",
        "    main()\n",
        "  File \"./mapper.py\", line 20, in main\n",
        "    print '%s%s%d' % (word, separator, 1)\n",
        "IOError: [Errno 32] Broken pipe\n",
        "cat: stdout: Broken pipe\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat pg20417.txt | ./mapper.py | sort | ./reducer.py | head "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"A\t2\n",
        "\"Alpha\t1\n",
        "\"Alpha,\"\t1\n",
        "\"An\t2\n",
        "\"And\t1\n",
        "\"BOILING\"\t2\n",
        "\"Batesian\"\t1\n",
        "\"Beta\t2\n",
        "\"Beta\"\t1\n",
        "\"Big\t1\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Traceback (most recent call last):\n",
        "  File \"./reducer.py\", line 27, in <module>\n",
        "    main()\n",
        "  File \"./reducer.py\", line 21, in main\n",
        "    print \"%s%s%d\" % (current_word, separator, total_count)\n",
        "IOError: [Errno 32] Broken pipe\n",
        "sort: write failed: standard output: Broken pipe\n",
        "sort: write error\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that python's `.split()` method is not a very good tokenizer as it fails to remove punctuation. NLTK has guidelines on how to create better tokenizers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you are satisfied, upload mapper and reducer to s3:\n",
      "```bash\n",
      "$ aws s3 cp mapper.py s3://<my_bucket>/\n",
      "$ aws s3 cp reducer.py s3://<my_bucket>/\n",
      "```\n",
      "Make sure everything is there:\n",
      "```bash\n",
      "$ aws s3 ls s3://<my_bucket>\n",
      "```\n",
      "should return something like:\n",
      "\n",
      "                               PRE gutenberg/\n",
      "    2014-06-07 19:25:57        540 mapper.py\n",
      "    2014-06-07 19:26:03       1034 reducer.py"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SSH into hadoop cluster once up and running:\n",
      "```bash\n",
      "$ ssh hadoop@ec2-XXX-XXX-XXX-XXX.us-west-2.compute.amazonaws.com -i ~/.ssh/<my_public_key>.pem\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Verify that Hadoop can see your files on S3:\n",
      "```bash\n",
      "hadoop@ip-XX-XX-XX-XX:~$ hadoop fs -ls s3n://<my_bucket>/gutenberg/\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, run your Hadoop streaming job:\n",
      "```bash\n",
      "hadoop@ip-XX-XX-XX-XX:~$ hadoop jar contrib/streaming/hadoop-streaming.jar \\\n",
      "    -input s3n://<my_bucket>/gutenberg/* \\\n",
      "    -output s3n://<my_bucket>/gutenberg-output \\\n",
      "    -mapper s3n://<my_bucket>/mapper.py map \\\n",
      "    -reducer s3n://<my_bucket>/reducer.py reduce \n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice how much longer this takes than doing it locally. Hadoop streaming is extreamly inefficient because it has to read and write to disk at every step of the way. EMR is especially inefficient because the disk, in each case, may be miles away from the CPU. That said, it does make jobs possible that would otherwise be impossible."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the output:\n",
      "```bash\n",
      "hadoop@ip-XX-XX-XX-XX:~$ hadoop fs -ls s3n://<my_bucket>/gutenberg-output\n",
      "```\n",
      "        Found 53 items\n",
      "        -rwxrwxrwx   1          0 2014-06-08 02:54 /gutenberg-output/_SUCCESS\n",
      "        -rwxrwxrwx   1      16601 2014-06-08 02:53 /gutenberg-output/part-00000\n",
      "        -rwxrwxrwx   1      17278 2014-06-08 02:53 /gutenberg-output/part-00001\n",
      "        -rwxrwxrwx   1      17231 2014-06-08 02:53 /gutenberg-output/part-00002\n",
      "        -rwxrwxrwx   1      17419 2014-06-08 02:53 /gutenberg-output/part-00003\n",
      "        -rwxrwxrwx   1      16259 2014-06-08 02:53 /gutenberg-output/part-00004\n",
      "        -rwxrwxrwx   1      17051 2014-06-08 02:53 /gutenberg-output/part-00005\n",
      "                                        .\n",
      "                                        .\n",
      "                                        .\n",
      "        -rwxrwxrwx   1      16660 2014-06-08 02:54 /gutenberg-output/part-00049\n",
      "        -rwxrwxrwx   1      17177 2014-06-08 02:54 /gutenberg-output/part-00050\n",
      "        -rwxrwxrwx   1      16510 2014-06-08 02:54 /gutenberg-output/part-00051\n",
      "    \n",
      "```bash\n",
      "hadoop@ip-XX-XX-XX-XX:~$ hadoop fs -cat s3n://<my_bucket>/gutenberg-output/part-00000 | head\n",
      "```\n",
      "        \"A\t2\n",
      "        \"Alpine-glow\"\t1\n",
      "        \"Ignoramus\":\t1\n",
      "        \"It\t3\n",
      "        \"O\"\t1\n",
      "        \"Year\"\t2\n",
      "        \"Your\t2\n",
      "        \"_in_\t1\n",
      "        \"grouse\t2\n",
      "        \"planetoids,\"\t1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}